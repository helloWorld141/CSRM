{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Analysis of the India Shadow Banking crisis of 2018-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Participating Group Members:\n",
    "\n",
    "Long Nguyen (nguyenlongphi1507@gmail.com)\n",
    "\n",
    "Dang Duy Nghia Nguyen (nghia002@e.ntu.edu.sg)\n",
    "\n",
    "Bodhisattya Roy (bodhisattya_roy@yahoo.in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "The effect of the India Shadow Banking crisis can be observed directly in the NIFTY50 index. Hence, in this report, we explored different approaches to see if we can predict the coming of the crisis. In particular, we are going to discuss two approaches: one involves modeling NIFTY50 price based on exogenous factors, and the other involve building a time series model out of NIFTY50 price.\n",
    "\n",
    "In the prior, we selected a number of macro economics and market data factors and analyzed how much of them explains the variance in NIFTY50 price and build a linear model out of it. In the latter, we utilized the autocorrelation property of NIFTY50 price time series and tried to predict the crisis from the past price itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import pandas_datareader.wb as wb\n",
    "import pandas_datareader as pdr\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY = 'IND'\n",
    "START = pd.to_datetime('2000-01-01')\n",
    "END = pd.to_datetime('2020-12-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modeling using exogenous factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a number of macro economics factors and market data. The comprehensive is as follow: Current account balance (BoP, current \\\\$US), Reserves and related items (BoP, current \\\\$US), Foreign direct investment, net inflows (BoP, current \\\\$US), Stocks traded, total value (current \\\\$US), Official exchange rate, LCU per USD, period average, Portfolio investment, bonds (PPG + PNG) (NFL, current \\\\$US), Broad money (\\% of GDP), Lending interest rate (\\%), Real interest rate (\\%), Interest payments (\\% of revenue), GDP per capita (constant 2010 \\\\$US), Official exchange rate (LCU per \\\\$US, period average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = wb.get_indicators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_ids = ['FM.LBL.BMNY.GD.ZS', 'BX.KLT.DINV.CD.WD', 'BN.CAB.XOKA.CD', 'BN.RES.INCL.CD', 'FR.INR.LEND', 'FR.INR.RINR', 'NY.GDP.PCAP.KD', 'GC.XPN.INTP.RV.ZS',\n",
    "              'CM.MKT.TRAD.CD', 'DT.NFL.BOND.CD', 'DPANUSLCU', 'PA.NUS.FCRF']\n",
    "factors = indicators.loc[indicators.id.isin(factor_ids)]\n",
    "factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty50_raw = pdr.data.get_data_yahoo('^NSEI', start=START, end=END)\n",
    "nifty50_raw.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n50 = nifty50_raw[['Adj Close']]\n",
    "n50.hvplot.line(title='NIFTY50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_factors(indicator, countries, start, end, title, time_axis='year'):\n",
    "    data = wb.download(indicator=indicator, country=countries, start=start, end=end)\n",
    "    data = data.reset_index()\n",
    "    plot = data.iloc[::-1,:].hvplot.line(x=time_axis, y=indicator, by='country', title=title)\n",
    "    return data, plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data = dict()\n",
    "factor_plots = dict()\n",
    "for idx, factor in factors.iterrows():\n",
    "    data, plot = plot_factors(factor.id, [COUNTRY], START, END, factor['name'], time_axis='year')\n",
    "    factor_data[factor.id] = data\n",
    "    factor_plots[factor.id] = plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Curve [width=550]\n",
    "\n",
    "total_plot = reduce(lambda acc, cur: acc + cur, list(factor_plots.values()))\n",
    "total_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df = pd.concat(list(factor_data.values()), axis=1)[['year'] + factor_ids]\n",
    "factors_df = factors_df.set_index(pd.DatetimeIndex(factors_df.year.iloc[:, 0], yearfirst=True)).drop('year', axis=1).shift(-1)\n",
    "factors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n50['year'] = n50.index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = n50.groupby('year').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = factors_df[(factors_df.index.year >= 2007) & (factors_df.index.year <= 2020)].interpolate(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdScaler = StandardScaler()\n",
    "X_std = stdScaler.fit_transform(X)\n",
    "cov_mat = np.cov(X_std.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.set(font_scale=1.5)\n",
    "hm = sns.heatmap(cov_mat,\n",
    "                 cbar=True,\n",
    "                 annot=True,\n",
    "                 square=True,\n",
    "                 fmt='.2f',\n",
    "                 annot_kws={'size': 12},\n",
    "                 yticklabels=factor_ids,\n",
    "                 xticklabels=factor_ids)\n",
    "plt.title('Covariance matrix showing correlation coefficients')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = OLS(y.values, X)\n",
    "result = ols.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared of 0.977 indicates there is a strong correlation between the exogenous data and the nifty50 price. Among the factor, we can see that the most significant factor is 'Lending Interest Rate' at 1%-level using Students T-test. However, this model is highly unstable due the the limit of macro economics data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Time series analysis on NIFTY50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we aimed at utilizing the autocorrelation characteristic of time series data to try to forecast the crisis before it happened. The model used is ARIMA where AR represents the correlation with its past values and MA represent the moving average term which tries to capture the idisyncratic shocks observed in financial markets. We can think of events like terrorist attacks, earnings surprises, sudden political changes, etc. as the random shocks affecting the asset price movements. ARMA and ARIMA models have both AR and MA terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n50 = n50[['Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = n50.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to check the ACF and PACF plot to determine the lag terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_acf = acf(x, nlags=100)\n",
    "plt.figure(figsize=(16, 7))\n",
    "#Plot ACF: \n",
    "plt.plot(lag_acf, marker=\"o\")\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(x)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(x)),linestyle='--',color='gray')\n",
    "plt.title('Autocorrelation Function')\n",
    "plt.xlabel('number of lags')\n",
    "plt.ylabel('correlation')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_pacf = pacf(x, nlags=100, method='ols')\n",
    "plt.figure(figsize=(16, 7))\n",
    "plt.plot(lag_pacf, marker=\"o\")\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(x)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(x)),linestyle='--',color='gray')\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "plt.xlabel('number of lags')\n",
    "plt.ylabel('correlation')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the graphs, there is a strong corellation to past value suggested by ACF. However PACF suggests that direct correlation is only up to 2 lags. So we will use 2 as the value of lag for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model = ARIMA(x, order=(2,0,1))\n",
    "model_fit = model.fit()\n",
    "# summary of fit model\n",
    "print(model_fit.summary())\n",
    "# line plot of residuals\n",
    "residuals = pd.DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "plt.show()\n",
    "# density plot of residuals\n",
    "residuals.plot(kind='kde')\n",
    "plt.show()\n",
    "# summary stats of residuals\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "X = x\n",
    "size = int(len(X) * 0.66)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "# walk-forward validation\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(2,0,1))\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate forecasts\n",
    "rmse = np.sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "# plot forecasts against actual outcomes\n",
    "plt.plot(test)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analyzing past data, we can prove that these crisis could have been predicted using quantitative methods and be intervened with the right measure to prevent it from happening. Learning from these past mishaps, regulations need to be strenghthened and made thorough to even prevent the precursor of such events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
